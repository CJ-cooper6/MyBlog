---
title: 大数据数据库操作实践
date: 2024-12-09 16:50:46
tags: 经验
---

# 大数据数据库操作实践



1、分批次处理大量数据而不是逐条处理

```python
def fetch_course_outlines_in_chunks(self, chunk_size=100, custom_course_nos = None):
        course_nos = self._fetch_all_course_nos()
        for i in range(0, len(course_nos), chunk_size): # 一次处理100条相关数据
        	_course_nos = course_nos[i: i + chunk_size]	
        	placeholders = ', '.join(['%s'] * len(_course_nos))
					course_dict = self._fetch_base_course_outlines(_course_nos, placeholders)
          course_dict = self._fetch_main_content(course_dict, _course_nos, placeholders)
          outlines = list(course_dict.values())
          yield _course_nos, outlines  # 使用python yield关键字，不需要一次性加载所有数据，返回支持迭代处理的数据，提供更好的内存管理
          
# 使用
for course_nos, outlines in fetch_course_outlines_in_chunks(custom_course_nos=custom_course_subject_codes):
  ...
```



2、更新数据库时使用批量操作替代循环操作

```python
# 参考
batch_size = 100
with db.atomic():
		for course_id, d in course_id_attributes_data_map.items():
  		json_str = json_dumps(d)
      db.execute_sql(
          "UPDATE course_attributes SET data = %s WHERE id = %s",
          (json_str, course_id)
      )
      Course.bulk_update(courses, fields=course_fields, batch_size=batch_size)
      CourseOutline.bulk_update(course_outlines, fields=course_outline_fields, batch_size=batch_size)

      CourseOutlineItem.bulk_update(
      		course_outline_items_to_update, fields=course_outline_items_fields, 
      		batch_size=batch_size
      )
      for item in course_outline_items_to_insert:
          item.save()
```



3、耗时操作添加重试机制，类似：

```python
def retry_query_n_times(self, _func, num=3, interval=20):
    num_ = 0
    while num_ < num:
        try:
            return _func()
        except Exception as e:
            time.sleep(interval)
            num_ += 1
```



4、预先获取需要的数据，避免重复查询



5、异常捕捉并记录详细日志





